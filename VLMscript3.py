# -*- coding: utf-8 -*-
"""GM-VLMScript.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OuoL2HQ6yxUyQACKJT5jiSnzVl5UKv6a
"""

!pip install opencv-python

!pip install -U git+https://github.com/huggingface/transformers.git
!pip install -U decord accelerate

# Focus on extracting video frames from Scania manufacturing video (small dataset):
# Video is 20:29 minutes. 1229/5 = ~245.8 (a very small dataset of 5s sequences). Can we consider audio in these video scenes? Hm. We want to predict and won't always have some sort of voice-over (i.e., context).
# Install dependencies:
!pip install yt-dlp transformers accelerate timm
!pip install -q opencv-python-headless
!pip install -q torchvision



# Download the Scania video from YouTube. Based on search, existing datasets appear to be for separate tasks (forklift specifically or exclusively door assembly - still need to find/construct a universal dataset).
#!yt-dlp -f mp4 "https://www.youtube.com/watch?v=Rm6grXvyX6I" -o "factory_video.mp4"

import yt_dlp
video_url = "https://www.youtube.com/watch?v=Rm6grXvyX6I"
output_filename = "factory_video.mp4"

ydl_opts = {
    'format':'bestvideo+bestaudio/best',
    'outtmpl': output_filename,
    'merge_output_format':'mp4'
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
  ydl.download([video_url])



import numpy as np
import pandas as pd
import cv2
import os

!apt-get -y install ffmpeg
!pip install -q yt_dlp

# Download YouTube video - Scania production (for example)

import cv2
print(cv2.__file__)
print(dir(cv2))

#pip uninstall opencv-python opencv-python-headless -y

#pip show opencv-python #nothing should be returned

#pip install opencv-python



''' Come back to this later?
import cv2
import os
from glob import glob

video_dir = "/content/clips"
frames_root = "/content/video_frames"
os.makedirs(frames_root, exist_ok=True)

def extract_frames(video_path, save_dir, num_frames=12):
    os.makedirs(save_dir, exist_ok=True)
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    interval = max(1, total_frames // num_frames)

    for i in range(num_frames):
        frame_idx = i * interval
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if ret:
            cv2.imwrite(os.path.join(save_dir, f"frame_{i:02d}.jpg"), frame)
        else:
            break
    cap.release()

# Run for all clips
video_files = sorted(glob(os.path.join(video_dir, "*.mp4")))
for video_path in video_files:
    clip_id = os.path.splitext(os.path.basename(video_path))[0]
    extract_frames(video_path, os.path.join(frames_root, clip_id))
'''

''' Come back to this later? https://github.com/salesforce/LAVIS
from lavis.models import load_model_and_preprocess
from PIL import Image
import torch
from glob import glob

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load pretrained BLIP-2 video model
model, vis_processors, _ = load_model_and_preprocess(
    name="blip2_video_caption",
    model_type="coco",
    is_eval=True,
    device=device
)
'''

# We can use Video-LLava for constructing natural language captions for 5-second clips, for instance. https://github.com/PKU-YuanGroup/Video-LLaVA

"""https://github.com/PKU-YuanGroup/Video-LLaVA"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/PKU-YuanGroup/Video-LLaVA.git
# %cd Video-LLaVA

# Install dependencies

!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install -q opencv-python decord einops
!pip install -q git+https://github.com/huggingface/transformers.git
!pip install -q git+https://github.com/huggingface/peft.git
!pip install -q git+https://github.com/huggingface/accelerate.git
!pip install -q sentencepiece bitsandbytes

!pip install -q transformers[torch] decord av einops accelerate

from transformers import VideoLlavaProcessor, VideoLlavaForConditionalGeneration

import torch

model_id = "LanguageBind/Video-LLaVA-7B-hf"

!pip install --upgrade tim
# In reference to error: ImportError: cannot import name 'ImageNetInfo' from 'timm.data' (/usr/local/lib/python3.11/dist-packages/timm/data/__init__.py)

processor = VideoLlavaProcessor.from_pretrained(model_id)
model = VideoLlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16)
model.to("cuda")



import av
import numpy as np

#specify video path

video_path - "/content/clips/scene_0005.mp4"

#Read the video frames from the clip
container = av.open(video_path)
frames = [frame.to_ndarray(format="rgb24") for frame in container.decode(video=0)]
frames = np.stack(frames[:30]) #extract first 30 frames... (why first 30? Review video-LLaVA paper.)

inputs = processor(frames=frames, return_tensors="pt").to("cuda")
prompt = "USER: <video> What is the worker doing in this video clip? ASSISTANT:"
inputs.update(processor(text=prompt, return_tensors="pt"))
inputs = {k: v.to("cuda") for k, v in inputs.items()}

#Create caption! Make an inference from video.

outputs = model.generate(**inputs, max_new_tokens=64)
print(processor.decode(outputs[0], skip_special_tokens=True))
